# Test Suite Changes

## Essential Changes

These are changes we need to make to improve testing, but they don't replace the existing framework.
Furthermore, as long as we have all reference data and tolerances defined in files, we can move to any other
regression framework in the future. 

### 1. Explicit Tolerances

We retain tolerances in a file, however we:
* Put tolerances for **all** quantities in this
* Put tolerances in a single json file that correspond to quantities in all ground state files
* A tolerance file with defaults should be autogenerated
  * Developers can then change defaults manually, where required (nothing assumed/implicit, or defined far away)
* Format is XML or JSON (my preference is pretty-formatted JSON), with no nesting except for `calculation`, which
contains git history, what compiler, executable and run settings where used to generate the file 
    * Maybe run details should be a separate file? 

```json
gs_tolerance.json = 
{ 
  'caculation': 
      {'compiler': 'Intel 2017',
       'executable': 'exiting_serial',
       'omp':  1, 
       'np':  1
      },
  
  'Kinetic energy':            {1.e-8: 'ha'},
  'Estimated fundamental gap': {1.e-8: 'ha'},
  'total_energy':              {1.e-8: 'ha'},
  'max_scf':                   {1    : 'None'}
}
```

### 2. Change Test Directory Structure and Remove Run Folders
* Scrap the run folders: Have the test suite generate them
* Add one level of subdirectory structure (and simplify the test names):
  * ground_state
  * ground_state_properties  
  * gw
  * bse
  * phonon
  * etc 

### 3. Add Integer Comparison 
* Need this for max SCF iterations, etc


### 4. Improve Error Reporting 

#### Essential
Write meaningful info and offer verbosity:
```text
E           lattice_constants
E           Max absolute difference: 1.e-06
E            actual: array([1.200001, 1.300001, 1.400001])
E            reference: array([1.2, 1.3, 1.4])
```

#### Future Proposals 
* For big files, it might be useful to have an option to pass the two files to a visual comparison tool like
kdiff3 or [BBedit](https://www.git-tower.com/blog/5-tips-for-bbedit-text-editor/).


* For things like DOS or band structure, visual comparison is way easier to spot issues, which means we need to sort parsers
and some lightweight matplotlib for overlaying two plots, or putting them side by side. 
  * These won't be in the test suite, but they can be added to the python3 tools


### 5. Make Sure All Parsing Is Done By the Parsers (or Parser Wrappers)

Make sure all parsing in the test suite is clearly separated (and then move the parsers). We can have wrappers that put the parsed data 
into a more usable format, if required. 

### 6. Split the Reference Generator from the Test Runner

Split the Reference Generator from the Test Runner:
* The code is easier to read. 
* We separate dependencies

All the reference generator needs to be is given an output directory, create a folder with:
* Copies of output files, with `.ref` extensions added
* Creates a default tolerances file for the calculation type 
* Gets the calculation meta-data (git hash, compiler, etc). Probably from INFO.OUT 

Additionally, we should have a wholesale refactoring tool that can:
* Change the keys of the tolerance files: Important if the parsers change and we don't have total control over the keys
* Redo reference data for specified file (without killing the tolerances file): This should **have** to take a file or list 
  of files.
* Maybe this is the same program   


### 7. Move to Iterating Over Nested Dicts
This repo is exploring ways of iterating over all elements of two nested dictionaries, and performing numerical comparisons 
on all hashable elements. This is important if:

* We change parsers (such as move to NOMAD), or
* Change the structure of outputs (planned as part of the I/O refactoring)

Both will cause our current code to break. The above is more robust, and works as long as we parse data into dictionaries. 
See `dict_tools.py` and `test_dict_tools.py` for some prototyping. 

This change really needs to happen as we're guaranteed to do one (or both) of the above in the near future. 


## Refactor Proposal 

The advantage of carrying all tolerances and reference data in files is that we're able to move to another regression
testing framework - all the required test data is retained. 

Do we want a fixed framework where people provide input files and everything gets compared, or do we want people to 
write their own lightweight test modules?

#### Regression framework
* Pros
  * Infrastructure code that does not scale with the number of tests
    * That should make it easier to maintain and refactor
  * Easier to write a regression test: Provide reference data and a tolerance file, that's it
  * *Relatively* straightforward to switch one regression framework for another

* Cons
  * Don't get the fine control offered by Ctest (unless we use Reframe, perhaps) for grouping test types,
  and coupling to the build system
  * More opaque than having a simple python file that is run with pytest
  * Not as good error reporting (compared to pytest)
  * Can't do more complex tests as described below (maybe this is not actually a bad thing)


#### Python File per Test
* Pros
  * Great reporting with pytest
  * Easy to integrate with ctest (and hence the build system)
  * Perform explicit and pure regression testing side by side (no need to choose)
    * Hence can be lighter on the file system 
  * Flexible testing: The ability to do more than just compare two numbers or arrays 
  * Easy to call pytest on single or groups of tests


* Cons
  * Difficult to refactor if we want to move to any other existing regression framework
    * Maybe not - scrap the python files and just use existing references in the new regression framework
  * Requires some work to make the changes
    * Adding classes for example (see below)


### Reframe Test Framework

ReFrame is like a CTest by itself. It can launch jobs on scheduler and retrieve the results. 
So you can run parallel tests on the HPC platform. Itâ€™s like Jube + a strong parsing and assert mechanism.
This would simply replace what we already have. Developed at ETH Zurich. 

* Anton mocked up an example for us.
* Does everything we want and doesn't require a complete refactor. More like swapping the current framework for 
  this whilst retaining our reference files.
* Downsides: High barrier to entry, more dependencies, not a defacto framework 


### pytest Test Framework

My suggestion. 

A bigger refactor involves changing the test framework. We scrap the runTests routines and have a python module per test:

```text
> ls groundstate/lda_silicon 

test_lda_silicon.py
gs_tolerances.json
INFO.OUT.ref
LATTICE.OUT.ref
etc 
```

where `test_lda_silicon.py` looks like:

```python
from exciting_tools.runner import setup, run_exciting
from exciting_tools.parse  import parse, parse_tolerances
from exciting_tools.test   import compare_result_with_reference


def setup_module(module):
      """
      Setup once per test module 
      """
      cmd_inputs = setup()   # Move reference files into run directory (i.e. everything ending in .ref)
                             # Maybe copy from a prior run (depends-on argument)
                             # Return argparse inputs 
      stdout, stderr = run_exciting(file_name, binary, options)


def test_info_out():
     # Parse reference data from that directory
     # Maybe parse several files into the same dict parse("INFO.OUT.ref", "LATTICE.OUT.ref")
     reference_data = parse("INFO.OUT.ref")   
     results = parse("INFO.OUT")   
     tolerances = parse_tolerances("gs_tolerances.json") 

     # God-function comparison
     compare_result_with_reference(result, reference, tolerances)
```

I like this idea because:
* We can also do explicit assertions in the same framework
* We can use all the features of ctest (labelling slow tests, failing tests, skipped tests etc)
* Dependencies are lightweight: CMake (the planned build system), numpy and pytest
* We can perform more sophisticated tests, for example read the DOS resample it, perform statistics etc

#### However
If we implement the above but also use explicit assertions, the refactoring following any
parser or output structure change will be hard. For example, 

```python
def test_info_out():
     tolerances = parse_tolerances("gs_tolerances.json") 
     results = parse("INFO.OUT")   
     
     last_scf_result =  results['ground_state']['scf']['10']
     kinetic_energy = last_scf_result['kinetic_energy']
     
     assert np.is_close(kinetic_energy, 1.000, atol=1.e-7, err_msg='kinetic_energy') 
```
may need to be refactored to :

```python

last_scf_result =  results['ground_state']['exciting_info_out']['scf']['10']
kinetic_energy = last_scf_result['ex_KineticEnergy']
```
For example, not just changes in key names, but also level of nesting (output structure). This can be mitigated by
* Having an intermediate step that puts the parsed information into objects:

```python
def test_info_out():
     results = parse("INFO.OUT")   
     gs_result = GroundState(dictionary=results)

     tolerances = parse_tolerances("gs_tolerances.json") 
    
     last_scf_energies = gs_result.get_scf(-1)
     ke = last_scf_energies.kinetic_energy
     
     assert np.is_close(ke, 1.000, atol=1.e-7, err_msg='kinetic_energy') 
```

But clearly this requires some work to write Python classes, and methods to initialise classes from parsed data
as well as methods get data (although that should be minimal). The positive being that these classes are unlikely 
to change as exciting's ground state is mature. 